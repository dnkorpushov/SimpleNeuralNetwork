{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что такое математический нейрон\n",
    "По аналогии с биологическим нейроном в 1943 году Уоррен Маккалок (Warren Sturgis McCulloch) и Уолтер Питтс (Walter Harry Pitts) создали математическую модель нейрона.\n",
    "\n",
    "Он имеет, в общем случае, множество входов и один выход. На входы нейрона поступают сигналы от внешних источников или с выходов других нейронов. Эти сигналы, как правило, должны быть нормализованы в заданных пределах, например, от 0 до 1. \n",
    "\n",
    "![Математический нейрон](neuron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Входные сигналы дальше проводятся в тело нейрона по синаптическим связям. Синапсы могут усиливать или ослаблять сигнал определённого входа, а также делать его влияние положительным или отрицательным, что достигается умножением сигнала на соответствующий синаптический (весовой) коэффициент. \n",
    "\n",
    "Взвешенные сигналы суммируются в теле нейрона, формируя таким образом его состояние. Выход нейрона получается в результате применения активационной (передаточной) функции к рассчитанному состоянию.  Полученный выход может быть как выходом нейронной сети в целом, так и подаваться на входы других слоёв нейронов, в том числе предыдущие слои.\n",
    "\n",
    "Главная цель функции активации - внедрение нелинейности в модель нейронной сети. Наличие нелинейности позволяет нейронным сетям разрабатывать сложные представления и функции на основе входных данных, что было бы невозможно при использовании простых линейных функциях, ведь композиция линейных функций есть линейная функция. Также, функция активации используется для нормализации значения выхода нейрона от 0 до 1 (в отдельных случаях от -1 до 1). В зависимости от типа значение функции активации может быть дискретно - 0 или 1 (функция ReLU), либо непрерывно изменяться от 0 до 1 (сигмоида).\n",
    "\n",
    "В зависимости от класса нейронной сети некоторые нейроны могут иметь всего один вход, а функция активации может отсутствовать, формально выдавая как результат значение аргумента.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "x = np.linspace(-10, 10, 400)\n",
    "y_sigmoid = sigmoid(x)\n",
    "y_derivative = sigmoid_derivative(x)\n",
    "\n",
    "plt.plot(x, y_sigmoid, label=\"σ(x) = 1 / (1 + e^(-x))\", color=\"blue\")\n",
    "plt.plot(x, y_derivative, label=\"σ'(x) = σ(x) * (1 - σ(x))\", color='red', linestyle='--')\n",
    "plt.title(\"График сигмоиды и ее производной\")\n",
    "plt.xlabel(\"Ось X\")\n",
    "plt.ylabel(\"Ось Y\")\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Процесс обучения\n",
    "Рассмотрим процесс обучения на примере нейрона с тремя входами и одним выходом. В качестве функции активации нейрона будем использовать сигмоиду.\n",
    "\n",
    "Перед началом обучения веса и смещение инициализируются случайным образом. Определям некоторое значение правильного ответа $y_{\\text{true}}$.\n",
    "\n",
    "### 1. Рассчитывается выход нейрона\n",
    "Нейрон принимает три входа $x_1, x_2, x_3$, умножает их на соответствующие веса $w_1, w_2, w_3$, добавляет смещение $b$ и применяет функцию активации сигмоида:\n",
    "\n",
    "$$\n",
    "y = \\sigma(w_1 \\cdot x_1 + w_2 \\cdot x_2 + w_3 \\cdot x_3 + b)\n",
    "$$\n",
    "\n",
    "где $\\sigma$ - сигмоида:\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "### 2. Вычисление ошибки (Loss)\n",
    "Ошибка вычисляется как квадрат разницы между предсказанием $y$ и правильным ответом $y_{\\text{true}}$:\n",
    "\n",
    "$$\n",
    "\\text{Ошибка} = (y - y_{\\text{true}})^2\n",
    "$$\n",
    "\n",
    "### 3. Определение градиента ошибки по весам и смещению\n",
    "Чтобы понять, как изменить веса, нужно вычислить, как ошибка зависит от каждого веса. Это делается с помощью цепного правила (chain rule) из математического анализа. Градиент — направление, в котором нужно изменить веса нейрона, чтобы уменьшить ошибку.\n",
    "\n",
    "Градиент ошибки по каждому весу и смещению вычисляется с помощью цепного правила.\n",
    "\n",
    "Для каждого веса $w_i$:\n",
    "$$\n",
    "\\frac{\\partial \\text{Ошибка}}{\\partial w_i} = 2(y - y_{\\text{true}}) \\cdot \\sigma'(z) \\cdot x_i\n",
    "$$\n",
    "\n",
    "Для смещения $b$:\n",
    "$$\n",
    "\\frac{\\partial \\text{Ошибка}}{\\partial b} = 2(y - y_{\\text{true}}) \\cdot \\sigma'(z)\n",
    "$$\n",
    "\n",
    "### 5. Обновление весов и смещения\n",
    "Веса и смещение обновляются с учетом градиента и скорости обучения $\\alpha$.\n",
    "\n",
    "Для каждого веса $w_i$:\n",
    "$$\n",
    "w_i = w_i - \\alpha \\cdot \\frac{\\partial \\text{Ошибка}}{\\partial w_i}\n",
    "$$\n",
    "\n",
    "Для смещения $b$:\n",
    "$$\n",
    "b = b - \\alpha \\cdot \\frac{\\partial \\text{Ошибка}}{\\partial b}\n",
    "$$\n",
    "\n",
    "### 6. Повторение с шага 1\n",
    "Этот процесс повторяется много раз, пока ошибка не станет достаточно маленькой.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция потерь (квадратичная ошибка)\n",
    "def loss(y_true, y_pred):\n",
    "    return 0.5 * (y_true - y_pred)**2\n",
    "\n",
    "# Градиент функции потерь по параметру x\n",
    "def gradient(y_true, y_pred, x):\n",
    "    return (y_pred - y_true) * sigmoid_derivative(x)\n",
    "\n",
    "# Параметры градиентного спуска\n",
    "learning_rate = 0.5  # Скорость обучения\n",
    "iterations = 1000     # Количество итераций\n",
    "x = 4.0              # Начальное значение x\n",
    "y_true = 0.8         # Целевое значение (желаемый выход сигмоиды)\n",
    "\n",
    "# Списки для хранения истории значений\n",
    "x_history = []\n",
    "loss_history = []\n",
    "\n",
    "# Градиентный спуск\n",
    "for i in range(iterations):\n",
    "    y_pred = sigmoid(x)  # Предсказанное значение\n",
    "    l = loss(y_true, y_pred)  # Значение функции потерь\n",
    "    grad = gradient(y_true, y_pred, x)  # Градиент\n",
    "    \n",
    "    # Обновляем x\n",
    "    x = x - learning_rate * grad\n",
    "    \n",
    "    # Сохраняем историю\n",
    "    x_history.append(x)\n",
    "    loss_history.append(l)\n",
    "\n",
    "# Визуализация\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# График функции потерь\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_history, label='Функция потерь')\n",
    "plt.xlabel('Итерация')\n",
    "plt.ylabel('Потери')\n",
    "plt.title('Градиентный спуск: Функция потерь')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# График изменения x\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x_history, label='x')\n",
    "plt.axhline(y=np.log(y_true / (1 - y_true)), color='red', linestyle='--', label='Оптимальное x')\n",
    "plt.xlabel('Итерация')\n",
    "plt.ylabel('Значение x')\n",
    "plt.title('Градиентный спуск: Изменение x')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример реализации нейронной сети на Python\n",
    "\n",
    "Объявляем класс `SimpleNeuralNetwork`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # Инициализация весов случайными значениями\n",
    "        self.weights = np.random.randn(3)\n",
    "        self.bias = np.random.randn()\n",
    "\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        # Функция активации - сигмоида\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        # Производная сигмоиды\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        # Прямой проход - вычисление выхода\n",
    "        network_input = np.dot(inputs, self.weights) + self.bias\n",
    "        return self.sigmoid(network_input)\n",
    "    \n",
    "\n",
    "    def train(self, training_inputs, training_outputs, epochs=1000, learning_rate=0.1):\n",
    "        # Обучение с помощью градиентного спуска\n",
    "        for _ in range(epochs):\n",
    "            # Прямой проход\n",
    "            outputs = self.predict(training_inputs)\n",
    "\n",
    "            # Вычисление ошибки\n",
    "            error = training_outputs - outputs\n",
    "\n",
    "            # Градиентный спуск\n",
    "            adjustments = error * self.sigmoid_derivative(outputs)\n",
    "            self.weights += learning_rate * np.dot(training_inputs.T, adjustments)\n",
    "            self.bias += learning_rate * np.sum(adjustments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем экземпляр класса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = SimpleNeuralNetwork()\n",
    "print(\"Веса:\", nn.weights, \"Смещение:\", nn.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим обучающие даннные. \n",
    "\n",
    "`training_inputs` - четыре примера входных данных на три входа\n",
    "\n",
    "`training_outputs` - ожидаемые выходные значения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_inputs = np.array([\n",
    "    [0, 0, 1],\n",
    "    [1, 1, 1],\n",
    "    [1, 0, 1],\n",
    "    [0, 1, 1], \n",
    "])\n",
    "\n",
    "training_outputs = np.array([0, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим процесс обучения. Количество \"эпох\" (итераций) - 5000, скорость обучения - 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.train(training_inputs, training_outputs, epochs=5000, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, изменились ли веса и смещение после обучения: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Веса после обучения:\", nn.weights, \"Смещение после обучения:\", nn.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестируем сеть по результатам обучения. В качестве входных данных подаем:\n",
    "- [1, 0, 0] - результат должен быть 1.\n",
    "- [1, 1, 0] - результат должен быть 0.\n",
    "- [0, 0, 0] - результат должен быть 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = np.array([1, 0, 0])\n",
    "print(\"Предсказание для [1, 0, 0]\", nn.predict(test_input))\n",
    "test_input = np.array([1, 1, 0])\n",
    "print(\"Предсказание для [1, 1, 0]\", nn.predict(test_input))\n",
    "test_input = np.array([0, 0, 0])\n",
    "print(\"Предсказание для [0, 0, 0]\", nn.predict(test_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что дальше?\n",
    "\n",
    "![Сова](sowa.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
